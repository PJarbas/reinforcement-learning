{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39abaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np \n",
    "import time, math, random\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a9981",
   "metadata": {},
   "source": [
    "### The Taxi Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918314d8",
   "metadata": {},
   "source": [
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"\n",
    "    by Tom Dietterich\n",
    "\n",
    "Description:\n",
    "\n",
    "    There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d49460",
   "metadata": {},
   "source": [
    "**Source**: https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca23d44",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "    There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "    Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n",
    "    \n",
    "    Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n",
    "    \n",
    "    This gives a total of 404 reachable discrete states.\n",
    "\n",
    "**Passenger locations:**\n",
    "\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    - 4: in taxi\n",
    "    \n",
    "**destinations:**\n",
    "\n",
    "    - 0: R(ed)\n",
    "    - 1: G(reen)\n",
    "    - 2: Y(ellow)\n",
    "    - 3: B(lue)\n",
    "    \n",
    "**Actions:**\n",
    "\n",
    "    There are 6 discrete deterministic actions:\n",
    "    - 0: move south\n",
    "    - 1: move north\n",
    "    - 2: move east\n",
    "    - 3: move west\n",
    "    - 4: pickup passenger\n",
    "    - 5: drop off passenger\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "    There is a default per-step reward of -1,\n",
    "    except for delivering the passenger, which is +20,\n",
    "    or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "    Rendering:\n",
    "    - blue: passenger\n",
    "    - magenta: destination\n",
    "    - yellow: empty taxi\n",
    "    - green: full taxi\n",
    "    - other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n",
    "**state space is represented by:**\n",
    "\n",
    "        (taxi_row, taxi_col, passenger_location, destination)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdbdfae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show info\n",
    "#?env.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad34c35",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ed3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.9):\n",
    "        self.env = env\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    def policy(self, state, epsilon):\n",
    "        if np.random.rand() > epsilon:\n",
    "            # Exploit\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            # Explore\n",
    "            action = np.random.randint(len(self.q_table[state]))\n",
    "        return action\n",
    "    \n",
    "    def exploration_rate(self, episode, num_episodes):\n",
    "        \"\"\"# Define exploration rate change over time\"\"\"\n",
    "        start_eps = 1.0\n",
    "        end_eps = 0.1\n",
    "        const_eps_episodes = 0.1 * num_episodes  # 10% of learning time\n",
    "        eps_decay_episodes = 0.6 * num_episodes  # 60% of learning time\n",
    "\n",
    "        if episode < const_eps_episodes:\n",
    "            return start_eps\n",
    "        elif episode < eps_decay_episodes:\n",
    "            # Linear decay\n",
    "            return start_eps - (episode - const_eps_episodes) / \\\n",
    "                               (eps_decay_episodes - const_eps_episodes) * (start_eps - end_eps)\n",
    "        else:\n",
    "            return end_eps\n",
    "\n",
    "    def load_q_table(self, table_filename):\n",
    "        table = np.load(table_filename)\n",
    "        self.q_table = table\n",
    "    \n",
    "    def print_frames(self, frames):\n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame'])\n",
    "            print(f\"Episode: {frame['episode']}\")\n",
    "            print(f\"Timestep: {i + 1}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    # train the agent for a given number of episodes\n",
    "    # the agent trains using Q-Learning\n",
    "    def train(self, training_episodes):\n",
    "        # count the number of steps per episode\n",
    "        # used to verify if the agent is learning\n",
    "        steps_per_episode = np.zeros((training_episodes))\n",
    "        run = 0\n",
    "        for i in range(0, training_episodes):            \n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            num_steps = 0\n",
    "            run+=1\n",
    "            \n",
    "            while not done:       \n",
    "                num_steps += 1\n",
    "                \n",
    "                #epsilon = self.exploration_rate(i, training_episodes)\n",
    "                #action = self.policy(state, epsilon)\n",
    "                \n",
    "                # select a greedy action from q-table\n",
    "                action = np.argmax(self.q_table[state])\n",
    "        \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                q_value = self.q_table[state, action]\n",
    "            \n",
    "                max_q = np.max(self.q_table[next_state])\n",
    "\n",
    "                new_q = q_value + self.learning_rate * (reward + self.discount_factor * max_q - q_value)\n",
    "                self.q_table[state, action] = new_q\n",
    "\n",
    "                state = next_state\n",
    "            \n",
    "            # total num of steps per episode\n",
    "            steps_per_episode[i] = num_steps\n",
    "        \n",
    "        print(\"\\nAvg num of steps per episode: \" + str(np.mean(steps_per_episode)))\n",
    "        plt.plot(steps_per_episode)\n",
    "        plt.show()  \n",
    "\n",
    "        outfile = open('q_table', 'wb')\n",
    "        np.save(outfile, self.q_table)\n",
    "        outfile.close()\n",
    "\n",
    "    # test the agent for a given number of episodes\n",
    "    # if render is active, it will print the steps and total reward\n",
    "    def test(self, testing_episodes, render=False):\n",
    "    \n",
    "        MAX_ITERS = 100\n",
    "        frames = []\n",
    "        total_penalties = 0\n",
    "        steps_per_episode = np.zeros((testing_episodes))\n",
    "        for i in range(testing_episodes):\n",
    "            \n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            steps = []\n",
    "            penalties, reward = 0, 0\n",
    "\n",
    "            num_iters = 0\n",
    "            \n",
    "            while not done and num_iters < MAX_ITERS:\n",
    "                action = np.argmax(self.q_table[state])\n",
    "                steps.append(action)\n",
    "                \n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "\n",
    "                num_iters += 1\n",
    "                \n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "        \n",
    "                # Put each rendered frame into dict for animation\n",
    "                frames.append({\n",
    "                    'frame': self.env.render(mode='ansi'),\n",
    "                    'episode': i, \n",
    "                    'state': state,\n",
    "                    'action': action,\n",
    "                    'reward': reward\n",
    "                    }\n",
    "                )\n",
    "                # total num of steps per episode\n",
    "                steps_per_episode[i] = num_iters\n",
    "                total_penalties += penalties\n",
    "        \n",
    "        if render:\n",
    "            self.print_frames(frames)\n",
    "            \n",
    "        print(f\"\\nResults after {testing_episodes} episodes:\")\n",
    "        print(f\"Average timesteps per episode: {np.mean(steps_per_episode)}\")\n",
    "        print(f\"Average penalties per episode: {total_penalties / testing_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d79bc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg num of steps per episode: 20.9457\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3').env\n",
    "agent = Agent(env)\n",
    "training_episodes = 10000\n",
    "agent.train(training_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9ec69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Episode: 9\n",
      "Timestep: 126\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n",
      "Results after 10 episodes:\n",
      "Average timesteps per episode: 12.6\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "testing_episodes = 10\n",
    "agent.test(testing_episodes, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d29701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
